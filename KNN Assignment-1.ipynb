{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "213db42c-aa6d-46d0-a029-c14a39013553",
   "metadata": {},
   "source": [
    "# KNN Assignment-1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a8de9f-5fb1-4eaa-b29e-544ffa3f5001",
   "metadata": {},
   "source": [
    "Q1. What is the KNN algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a51092-2c0c-455f-9bde-8fda9e50823b",
   "metadata": {},
   "source": [
    "The K-Nearest Neighbors (KNN) algorithm is a simple machine learning algorithm used for classification and regression tasks. It predicts the class or value of a new data point by looking at the majority class or averaging values among its K nearest neighbors in the feature space. It's non-parametric and doesn't assume any underlying data distribution. The choice of K is a crucial hyperparameter, and it computes distances between data points to find the nearest neighbors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f6aef66-3826-4402-8435-97e8d6bc97c9",
   "metadata": {},
   "source": [
    "Q2. How do you choose the value of K in KNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d50daead-f8d4-466c-aa40-5d55420cf658",
   "metadata": {},
   "source": [
    "Start with \n",
    "K= \n",
    "n\n",
    "‚Äã\n",
    " , where \n",
    "ùëõ\n",
    "n is the number of data points.\n",
    "\n",
    "Use cross-validation or grid search to find the best K based on performance metrics.\n",
    "\n",
    "Incorporate domain knowledge if available.\n",
    "\n",
    "Experiment with different K values to find the optimal balance between bias and variance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e06a4ffd-1f8a-42db-a617-6908a74a16d1",
   "metadata": {},
   "source": [
    "Q3. What is the difference between KNN classifier and KNN regressor?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55700204-0b47-432e-80e8-2f239001de30",
   "metadata": {},
   "source": [
    "1) KNN classifier predicts class labels for new data points.\n",
    "\n",
    "2) Knn regressor predicts continuous values for new data points."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ae1cc6-8439-44ec-bac2-9626390e84a1",
   "metadata": {},
   "source": [
    "Q4. How do you measure the performance of KNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d539c860-6973-45a9-bd06-7a0eeffa67bf",
   "metadata": {},
   "source": [
    "1) For classification tasks: Accuracy, precision, recall, F1 score, ROC curve, AUC, and confusion matrix.\n",
    "\n",
    "2) For regression tasks: Mean Absolute Error (MAE), Mean Squared Error (MSE), Root Mean Squared Error (RMSE)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c26154-3ffc-4728-be8c-9b1185198a3c",
   "metadata": {},
   "source": [
    "Q5. What is the curse of dimensionality in KNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f862df-6b27-4ec6-af0b-f4e51d6d2a34",
   "metadata": {},
   "source": [
    "the curse of dimensionality in KNN refers to decreased performance as the number of dimensions (features) increases. It leads to increased sparsity, diminished discriminative power, higher computational complexity, and increased risk of overfitting. Techniques like feature selection, dimensionality reduction, and careful feature engineering can help mitigate its effects.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd2319f0-a088-4d82-9503-6965ceb37ed6",
   "metadata": {},
   "source": [
    "Q6. How do you handle missing values in KNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "085087f2-b31b-4fc7-8782-dba2af74d0d9",
   "metadata": {},
   "source": [
    "1) Imputation: Replace missing values with estimated values using techniques like mean, median, mode, or KNN-based imputation.\n",
    "\n",
    "2) Deletion: Remove data points with missing values, but this may lead to loss of information.\n",
    "\n",
    "3) Feature Engineering: Create additional binary indicator variables to denote missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f56bc6b-0b20-4f73-a01d-b1be67a9b153",
   "metadata": {},
   "source": [
    "Q7. Compare and contrast the performance of the KNN classifier and regressor. Which one is better for\n",
    "which type of problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e5c634d-fe9a-47fa-a2ff-03d302cb0730",
   "metadata": {},
   "source": [
    "Use KNN classifier for classification tasks with discrete outcomes.\n",
    "\n",
    "Use KNN regressor for regression tasks with continuous outcomes.\n",
    "\n",
    "Consider the dataset characteristics and experiment to determine which performs better for a specific problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c2fc0da-ef46-40a1-8ef9-4f858e92126b",
   "metadata": {},
   "source": [
    "Q8. What are the strengths and weaknesses of the KNN algorithm for classification and regression tasks,\n",
    "and how can these be addressed?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff66b96-38a8-4bf8-a559-36ae9bd38d60",
   "metadata": {},
   "source": [
    "1) Strengths:\n",
    "\n",
    ". For classification: effective for non-linear decision boundaries, simple, and can handle multi-class problems.\n",
    "\n",
    ". For regression: intuitive, easy to implement, and robust to outliers.\n",
    "\n",
    "2) Weaknesses:\n",
    "\n",
    ". For classification: computationally expensive, sensitive to irrelevant features, requires careful hyperparameter selection, and may struggle with imbalanced datasets.\n",
    ". For regression: performance can suffer in high-dimensional data, less interpretable compared to linear regression, and may struggle with nonlinear relationships.\n",
    "\n",
    "3) Addressing Weaknesses:\n",
    "\n",
    "Use dimensionality reduction techniques like PCA.\n",
    "\n",
    "Tune hyperparameters via cross-validation.\n",
    "\n",
    "Detect and handle outliers appropriately.\n",
    "\n",
    "Consider ensemble methods for improved performance.\n",
    "\n",
    "Choose an appropriate distance metric."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44fb2e8b-3b69-4342-9dc0-141ee318472b",
   "metadata": {},
   "source": [
    "Q9. What is the difference between Euclidean distance and Manhattan distance in KNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd0dcbf1-71fb-4443-85de-d1b083343a6b",
   "metadata": {},
   "source": [
    "1) Euclidean distance measures the straight-line distance between two points, treating all dimensions equally.\n",
    "\n",
    "2) Manhattan distance measures distance by summing the absolute differences along each dimension, considering grid-like paths.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f4273b1-e773-43a1-9f3c-02dc3a2b414b",
   "metadata": {},
   "source": [
    "Q10. What is the role of feature scaling in KNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f7d0990-2a5c-4fa5-966f-7b1e789c3fe7",
   "metadata": {},
   "source": [
    " feature scaling in K-Nearest Neighbors (KNN) ensures that all features contribute equally to the distance calculation, improving the algorithm's performance. It involves transforming features to have a similar scale, typically using methods like min-max scaling or standardization. This helps mitigate biases caused by features with larger scales and ensures fair comparisons between features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2fd46e6-8eac-4645-a862-e454058d9243",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
